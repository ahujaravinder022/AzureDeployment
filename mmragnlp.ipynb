{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahujaravinder022/AzureDeployment/blob/master/mmragnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib\n"
      ],
      "metadata": {
        "id": "5RxEIIszex8-",
        "outputId": "510be559-4bea-443a-ab27-76cbfeac86ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.9.5)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.25.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.18.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.7.4)\n",
            "Requirement already satisfied: unstructured-inference==0.7.36 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.7.36)\n",
            "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.1.2)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (9.1.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: python-pptx<=0.6.23 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.23)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.13)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.3.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.3.10)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.1)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.0.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.6)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.17.0)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.3.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.1.4)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (20231228)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.0.9)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.23.5)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.10.0.84)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (1.18.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (1.0.8)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.42.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (3.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.18.1+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.19.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2024.5.15)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (42.0.8)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.14)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (2024.7.4)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[all-docs]) (4.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.14.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36->unstructured[all-docs]) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.15.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (12.5.82)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36->unstructured[all-docs]) (0.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (1.13.1)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (0.11.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (2.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36->unstructured[all-docs]) (2.1.5)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (4.30.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "id": "1gr4sz-MeyGP",
        "outputId": "20d52978-ce5f-4bd4-d4d0-7fa366dd017d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [861 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,219 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,112 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,393 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,553 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [49.2 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,882 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,780 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,130 kB]\n",
            "Fetched 24.7 MB in 3s (9,651 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "c0usk6AseyJT",
        "outputId": "8f9a8a93-c1c2-4791-95a3-05f4dbf21965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn\n"
      ],
      "metadata": {
        "id": "ESAcojZteyM6",
        "outputId": "047f8ae8-2fa3-4d1d-c59d-a63d6343f662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libleptonica-dev is already the newest version (1.82.0-3build1).\n",
            "libleptonica-dev set to manually installed.\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "tesseract-ocr-eng set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libimagequant0 libraqm0 mailcap mime-support python3-olefile\n",
            "Suggested packages:\n",
            "  python-pil-doc\n",
            "The following NEW packages will be installed:\n",
            "  libimagequant0 libraqm0 mailcap mime-support python3-olefile python3-pil\n",
            "  tesseract-ocr-script-latn\n",
            "0 upgraded, 7 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 31.4 MB of archives.\n",
            "After this operation, 91.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraqm0 amd64 0.7.0-4ubuntu1 [11.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-olefile all 0.46-3 [33.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pil amd64 9.0.1-1ubuntu0.3 [419 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-latn all 1:4.00~git30-7274cfa-1.1 [30.9 MB]\n",
            "Fetched 31.4 MB in 2s (20.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "(Reading database ... 123798 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libimagequant0_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.17.0-1) ...\n",
            "Selecting previously unselected package libraqm0:amd64.\n",
            "Preparing to unpack .../1-libraqm0_0.7.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package python3-olefile.\n",
            "Preparing to unpack .../4-python3-olefile_0.46-3_all.deb ...\n",
            "Unpacking python3-olefile (0.46-3) ...\n",
            "Selecting previously unselected package python3-pil:amd64.\n",
            "Preparing to unpack .../5-python3-pil_9.0.1-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-latn.\n",
            "Preparing to unpack .../6-tesseract-ocr-script-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up python3-olefile (0.46-3) ...\n",
            "Setting up libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Setting up libimagequant0:amd64 (2.17.0-1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured-pytesseract\n",
        "!pip install tesseract-ocr"
      ],
      "metadata": {
        "id": "ybrd0Yw9givz",
        "outputId": "517cabc0-f12a-42c2-ab6d-5bef629fee04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured-pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured-pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-pytesseract) (10.4.0)\n",
            "Collecting tesseract-ocr\n",
            "  Downloading tesseract-ocr-0.0.1.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from tesseract-ocr) (3.0.10)\n",
            "Building wheels for collected packages: tesseract-ocr\n",
            "  Building wheel for tesseract-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract-ocr: filename=tesseract_ocr-0.0.1-cp310-cp310-linux_x86_64.whl size=169746 sha256=e6bb61af8318e409d167e8b547f83c41ccac81657dd8a002007ce7d44c8c91a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/fd/f3/5c231ecbbb80a1fe33204ff3021d99b54ef6daf6f8099311b8\n",
            "Successfully built tesseract-ocr\n",
            "Installing collected packages: tesseract-ocr\n",
            "Successfully installed tesseract-ocr-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf"
      ],
      "metadata": {
        "id": "25FKZPxXgzZ7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements=partition_pdf(\n",
        "    filename=\"/content/RAG-NLP.pdf\",\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data\"\n",
        "  )"
      ],
      "metadata": {
        "id": "zSn7gpKGh2A-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements"
      ],
      "metadata": {
        "id": "xYcjf7Eqh5Rp",
        "outputId": "7eaa5d9e-5cbb-4e66-9448-3e2e59c8ca2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Text at 0x7f373d7b5cf0>,\n",
              " <unstructured.documents.elements.Header at 0x7f35ac7b45b0>,\n",
              " <unstructured.documents.elements.Text at 0x7f35ac9effd0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a57b9c30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac7b47f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac7b4640>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac7b4910>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac9eea10>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac992d40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac990430>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac3d40d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3d4100>,\n",
              " <unstructured.documents.elements.Image at 0x7f35ac3d4040>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c2830>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c1f90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c1540>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c1de0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c2800>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac3c0a30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c2020>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c0760>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35ac3c26b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c0640>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c0400>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac3c14b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c10f0>,\n",
              " <unstructured.documents.elements.Formula at 0x7f35ac3c24d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c1e70>,\n",
              " <unstructured.documents.elements.Formula at 0x7f35ac3c0ac0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c34c0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac3c1360>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c3700>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9ebc0>,\n",
              " <unstructured.documents.elements.Formula at 0x7f35ac3c2e90>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c2980>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac3c23e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac3c0370>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac3c0fa0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac1a83a0>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35ac1aa830>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40cc10>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40ea10>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35aca9d660>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40dd50>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40e4a0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40e0e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40dd80>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40fc40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40f6d0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40f9d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40df60>,\n",
              " <unstructured.documents.elements.Text at 0x7f35aca9c430>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40fd30>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40f2e0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40dcc0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40e770>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40fbb0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40d9f0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40ef50>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac40d990>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40d270>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac40d8d0>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35ac40dba0>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7f35ac40e410>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9d630>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9f7c0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9c7c0>,\n",
              " <unstructured.documents.elements.Table at 0x7f35ac40f700>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9d510>,\n",
              " <unstructured.documents.elements.Text at 0x7f35aca9f700>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54708e0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5472e00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5472b90>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5472dd0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54732b0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5471720>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a54715a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5473610>,\n",
              " <unstructured.documents.elements.Text at 0x7f35aca9d090>,\n",
              " <unstructured.documents.elements.Image at 0x7f35a5473280>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54730d0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5471960>,\n",
              " <unstructured.documents.elements.Table at 0x7f35a5472200>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5471b10>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5471c00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5473d30>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54703a0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54704c0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5473970>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a5470370>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5470c10>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9e050>,\n",
              " <unstructured.documents.elements.Table at 0x7f35a5470910>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7f35a5470be0>,\n",
              " <unstructured.documents.elements.Table at 0x7f35a5471480>,\n",
              " <unstructured.documents.elements.Title at 0x7f35aca9d540>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5470fa0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5470f40>,\n",
              " <unstructured.documents.elements.Image at 0x7f35a5472170>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7f35a5470970>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5471de0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5472080>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a5471d80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5472260>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5472380>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5472590>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54728f0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5472c80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5470880>,\n",
              " <unstructured.documents.elements.Text at 0x7f35aca9c340>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5471000>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5472680>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a5421180>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5420c40>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a54212d0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a5420b80>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5420c70>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5420e50>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5420b50>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421e10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5422230>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421f30>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a5422d70>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5423a90>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421630>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54232e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5422e90>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5422e30>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421ae0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421b10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421990>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54231c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5423400>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5422110>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54214b0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54219c0>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a5423280>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5421db0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54210c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5420ca0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54237f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54228c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5423190>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a54215d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5422f80>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a5422dd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35aca9d210>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35aca9d5a0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35aca9e740>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35aca9c730>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35aca9c2e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35aca9c250>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35aca9d480>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f640>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535e4d0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535edd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535ea40>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535ef20>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f700>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535e0b0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535fc70>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f8e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535e950>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a535ec20>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a535e560>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f4f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f730>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f4c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f940>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535e860>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535e680>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535fc10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535fd90>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535e2f0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a535f580>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d1780>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d0820>,\n",
              " <unstructured.documents.elements.Text at 0x7f35aca9c580>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d08b0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d1840>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d22c0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d20e0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d1e10>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d3220>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d2650>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d2dd0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d0100>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d1ed0>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a55d1d80>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d2e00>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d2fe0>,\n",
              " <unstructured.documents.elements.ListItem at 0x7f35a55d0b50>,\n",
              " <unstructured.documents.elements.Header at 0x7f35a55d04c0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d3fd0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d0340>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d1cc0>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d0d60>,\n",
              " <unstructured.documents.elements.Image at 0x7f35a55d1180>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d0be0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d2920>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d16f0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d3910>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d0d90>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a55d3d00>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d0d00>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d1270>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d3280>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d2740>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d2ef0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d0250>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d3700>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d0280>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d2ad0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d1d50>,\n",
              " <unstructured.documents.elements.Footer at 0x7f35a55d3a30>,\n",
              " <unstructured.documents.elements.FigureCaption at 0x7f35a55d0220>,\n",
              " <unstructured.documents.elements.Table at 0x7f35a55d3e80>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35a55d3850>,\n",
              " <unstructured.documents.elements.Title at 0x7f35a55d0520>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac7bea70>,\n",
              " <unstructured.documents.elements.Title at 0x7f35ac7bebf0>,\n",
              " <unstructured.documents.elements.NarrativeText at 0x7f35ac7bd5a0>,\n",
              " <unstructured.documents.elements.Header at 0x7f35ac9038b0>]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
        "            img.append(str(element))"
      ],
      "metadata": {
        "id": "moY4trz-iWFT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "JVgnVWHlia76",
        "outputId": "b574ebaa-ee13-48b7-d141-bcfd0ca534c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@--- ee ee ee ee ee ee ee ee ee ee eee The middle ear includes End-to-End Backprop through q and pe the tympanic cavity and the three ossicles. (y) Define \"middle ear\" (x) Question Answering: Question Query Retriever py Document Generator pg “fnower Generation Index. (Non-Parametric) (Parametric) d(z) supports (y) Barack Obama was born in Hawaii. (x) q(x) Fact Verification: Fact Query Fact Verification: Label Generation The Divine This 14th century work Comedy (x) is divided into 3 Jeopardy Question Generation: Answer Query sections: \"Inferno\", \"purgatorio\" & \"Paradiso\" @) Question Generation',\n",
              " 'Document 1: his works are considered classics of American Doc 1 | | literature ... His wartime experiences formed the basis for his novel poe 2 | | “A Farewell to Arms” (1929) ... Doc 3 Document 2: ... artists of the 1920s “Lost Generation” expatriate Doe 4 community. His debut novel, \"The Sun Also Rises”, was published °° in 1926. Doc 5 & , es ee £ te os & ss . TES eS',\n",
              " 'Bee TT % 80 Porm Sa SRS nana ga g / Z fr = 70 2 | / 3 RAG TORRE 2 nf |! g <= RAG-Tok B-1 Ba Ze H=- RAGSeq RL a ; 3 Zs == RAG-Seq BA Q 50 > 50 ZO — reactor | & 3 soft === RAGSeq | Z 40 2 4s rr rr rr nr) rr nr K Retrieved Docs K Retrieved Docs K Retrieved Docs',\n",
              " 'View full instructions Which sentence is more factually true? View tool guide Select an option Subject : Hemingway eI Note: Some questions are Sentence Ais more 1 control questions. We require Sentence A : \"The Sun Also Rises\" is a novel by this author of \"A true good accuracy on our control Farewell to Arms\" Sentence Bis more 2 questions to accept true responses. Sentence B : This author of \"The Sun Also Rises\" was born in Both sentences are 8 Havana, Cuba, the son of Spanish immigrants ‘rue Indicate which one of the P a following sentences is more Both sentences are factually true with respect to completely untrue the subject. Using the internet to check whether the sentences are true is encouraged.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "            tab.append(str(element))"
      ],
      "metadata": {
        "id": "2Z_VdiK3ids8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab[0]"
      ],
      "metadata": {
        "id": "zzfGh4FTihQf",
        "outputId": "fd59314d-5b51-4c24-f216-1289c54a9ae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - - /50.1 37.4 /60.5 44.7 - - Model B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - - - 40.7 46.8 41.1 50.6 SotA BART - - 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText.append(str(element))"
      ],
      "metadata": {
        "id": "oiHrS--hikcS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem.append(str(element))"
      ],
      "metadata": {
        "id": "-J3mEuQXioOw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText"
      ],
      "metadata": {
        "id": "-bPCS6lyirnT",
        "outputId": "c677ab64-e204-44a6-c776-b4c7271b3a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Patrick Lewis't, Ethan Perez*,\",\n",
              " 'Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,',\n",
              " 'Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†',\n",
              " 'Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
              " 'Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,',\n",
              " 'Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.',\n",
              " 'but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.',\n",
              " 'We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.',\n",
              " 'There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.',\n",
              " 'Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera- tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1',\n",
              " 'We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized',\n",
              " '1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/',\n",
              " 'by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z.',\n",
              " 'To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.',\n",
              " 'RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,',\n",
              " 'RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:',\n",
              " 'Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.',\n",
              " 'The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:',\n",
              " 'where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.',\n",
              " 'The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.',\n",
              " 'We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we',\n",
              " 'minimize the negative marginal log-likelihood of each target, ar — log p(y;|a;) using stochastic gradient descent with Adam [28]. Updating the document encoder BERT, during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERT, and the BART generator.',\n",
              " 'At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).',\n",
              " 'RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: pj (yi|z, yii—1) = zetop-k(p(-|2)) Pn (zil@)po(yil@, Zi, Y1e—1) To decode, we can plug Po(yi |x, y1i—1) into a standard beam decoder.',\n",
              " 'RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”',\n",
              " 'We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.',\n",
              " 'Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.',\n",
              " 'RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat',\n",
              " 'MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.',\n",
              " 'To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.',\n",
              " 'We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good.',\n",
              " 'FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.',\n",
              " 'Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.',\n",
              " 'There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading',\n",
              " 'to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.',\n",
              " 'As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).',\n",
              " 'Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model.',\n",
              " 'Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.',\n",
              " 'Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.',\n",
              " 'Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem- ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".',\n",
              " 'Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.',\n",
              " 'For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.',\n",
              " 'Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing any diversity-promoting decoding.',\n",
              " 'Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.',\n",
              " 'We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.',\n",
              " 'Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed',\n",
              " 'Table 4: Human assessments for the Jeopardy Question Generation Task.',\n",
              " 'between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.',\n",
              " 'Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in performance between them. We have the ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.',\n",
              " 'Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.',\n",
              " 'General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models.',\n",
              " 'Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be ﬁne-tuned for strong performance on a variety of tasks.',\n",
              " 'Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].',\n",
              " 'Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.',\n",
              " 'In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.',\n",
              " 'This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.',\n",
              " 'With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against misleading content and automated spam/phishing.',\n",
              " 'The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.',\n",
              " 'for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/ anthology/P19-1612.',\n",
              " 'approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf.',\n",
              " 'For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.',\n",
              " 'Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".',\n",
              " 'Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instruc- tions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.',\n",
              " 'We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision ﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼ 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/ blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/',\n",
              " '2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers',\n",
              " 'For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur in top 1000 documents for the query.',\n",
              " 'CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres- sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.',\n",
              " 'TriviaQA Evaluation setups The open-domain QA community customarily uses public develop- ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being simpler to answer from Wikipedia.',\n",
              " 'For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The ﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.',\n",
              " 'We experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document mechanisms may not be necessary for RAG.',\n",
              " 'Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable',\n",
              " 'parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating point precision to manage memory and disk footprints.',\n",
              " 'In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.',\n",
              " 'The number of training, development and test datapoints in each of our datasets is shown in Table 7.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ListItem"
      ],
      "metadata": {
        "id": "limmYCdait54",
        "outputId": "d79ab5e8-276f-40b4-c7d6-8964e509df29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http: //arxiv.org/abs/1611.09268. arXiv: 1611.09268.',\n",
              " '[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20.',\n",
              " '[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ D13-1160.',\n",
              " '[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod- ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.',\n",
              " '[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171.',\n",
              " '[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.',\n",
              " '[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre- hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.',\n",
              " '[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.',\n",
              " '[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz- ard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.',\n",
              " '[10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.',\n",
              " '[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/ P18-1082.',\n",
              " '[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/ anthology/P19-1346.',\n",
              " '[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id= H1gx1CNKPH.',\n",
              " '[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.',\n",
              " '[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/ AAAI/AAAI18/paper/view/16710.',\n",
              " '[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.',\n",
              " '[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural In AAAI Conference on Artiﬁcial Intelligence, 2018. URL https: machine translation. //www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.',\n",
              " '[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.',\n",
              " '[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.',\n",
              " '[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https: //arxiv.org/abs/2002.08909.',\n",
              " 'A In S. Bengio, retrieve-and-edit H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed- itors, Advances pages 10052– 10062. Curran Associates, URL http://papers.nips.cc/paper/ 8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs. pdf.',\n",
              " '[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve- edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa- tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/ anthology/2020.acl-main.228.',\n",
              " '[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.',\n",
              " '[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.',\n",
              " 'Inferring algorithmic patterns with stack- the 28th International Conference on augmented recurrent nets. Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam- bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/ 5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.',\n",
              " '[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.',\n",
              " '[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza- tion through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.',\n",
              " '[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.',\n",
              " '[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken- ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques- the Association of Computational Lin- tion Answering Research. guistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/ natural-questions/main-1455-kwiatkowski.pdf.',\n",
              " '[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In- formation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http: //papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.',\n",
              " '[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association',\n",
              " '[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.',\n",
              " '[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/ N16-1014.',\n",
              " '[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.',\n",
              " '[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.',\n",
              " '[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=Hyg0vbWC-.',\n",
              " '[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.',\n",
              " '[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.',\n",
              " '[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the arXiv preprint arXiv:1911.03587, 2019. URL https: veriﬁability of generated text. //arxiv.org/abs/1911.03587.',\n",
              " '[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.',\n",
              " '[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit- ing background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus- sels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.',\n",
              " '[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/ anthology/D18-1429.',\n",
              " '[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic',\n",
              " '[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.',\n",
              " '[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb. org/anthology/N19-4009.',\n",
              " '[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.',\n",
              " '[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1250. URL https://www.aclweb.org/anthology/D19-1250.',\n",
              " '[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum? id=025X0zPfn.',\n",
              " '[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. proving Language Understanding https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/ language-unsupervised/language_understanding_paper.pdf. by Generative Pre-Training, 2018. Im- URL',\n",
              " '[50] Alec Radford, Sutskever. https://d4mucfpksywv.cloudfront.net/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf.',\n",
              " '[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.',\n",
              " '[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/ 2002.08910.',\n",
              " '[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/ 1500000019. URL https://doi.org/10.1561/1500000019.',\n",
              " '[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.',\n",
              " '[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net- works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.',\n",
              " '[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074.',\n",
              " '[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.',\n",
              " '[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.',\n",
              " '[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17329.',\n",
              " '[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/ anthology/W18-5446.',\n",
              " '[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General- Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https:// arxiv.org/abs/1905.00537.',\n",
              " '[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/16712.',\n",
              " '[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re- ranking in open-domain question answering. In ICLR, 2018. URL https://openreview. net/forum?id=rJl3yM-Ab.',\n",
              " '[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.',\n",
              " '[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713.',\n",
              " '[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.',\n",
              " '[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi- supervised question answering. In Proceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem- ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253.',\n",
              " '[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "8IOhgLqHixLp",
        "outputId": "34ece25a-779d-45c1-bef3-48dd8089043a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.24 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.25)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.37.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain_openai) (0.1.94)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain_openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.24->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.24->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.24->langchain_openai) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.24->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.24->langchain_openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.19-py3-none-any.whl (47 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.1.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core\n"
      ],
      "metadata": {
        "id": "NBuf0t4Ui1HZ",
        "outputId": "d648b690-4129-46c4-fd38-e56c51269e82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.25)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.94)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tab)"
      ],
      "metadata": {
        "id": "g3CVXMdni7m9",
        "outputId": "01f01017-41d3-4280-c6b1-2d6112a33472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab[0]"
      ],
      "metadata": {
        "id": "lsXkuLLMjAKp",
        "outputId": "de3f0f22-f9c5-4e6e-f382-9086608bc20d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - - /50.1 37.4 /60.5 44.7 - - Model B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - - - 40.7 46.8 41.1 50.6 SotA BART - - 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(img)"
      ],
      "metadata": {
        "id": "cUxNGoFJjDB5",
        "outputId": "edd3dc33-79b2-4169-a388-460b35ca1f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "HxHtGLXBjGLn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
        "    Give a concise summary of the table that is well optimized for retrieval. Table {element} \"\"\""
      ],
      "metadata": {
        "id": "YUm434g5jI2e"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(prompt_text)"
      ],
      "metadata": {
        "id": "NqhbmaYAjLb-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "OPENAI_API_TOKEN=userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
      ],
      "metadata": {
        "id": "GiUdaHdnjPEF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "-eUSrOM2jR1m"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "hQL7q4vYjV6N"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries = []"
      ],
      "metadata": {
        "id": "9jjDqrVajYRy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries=summarize_chain.batch(tab,{\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "WIsMV16Tjbc5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab[0]\n",
        "len(tab)"
      ],
      "metadata": {
        "id": "6ucehs4XjnpX",
        "outputId": "ceb9d2df-7de7-435c-bc95-3ac0e7b28b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries"
      ],
      "metadata": {
        "id": "_2xKHbhjjq4m",
        "outputId": "8d2b7ad5-5225-48cb-ac90-a6ae97876047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The table presents the performance of various models on a task, measured by accuracy. Models include Closed Book T5-11B, T5-11B+SSM, Open Book REALM, DPR, SotA BART, RAG-Token, RAG-Seq, and RAG-Tok. The performance is measured in different settings, with some models having multiple measurements. The highest accuracy is achieved by RAG-Seq with 89.5, followed by SotA BART with 81.1. The lowest accuracy is achieved by RAG-Tok with 14.7.',\n",
              " \"The table provides different model-generated responses to various prompts. The prompts include defining the middle ear, identifying the currency used in Scotland, the U.S. state with the most counties, and details about Dante's epic poem. The models used are BART, RAG-T, RAG-S, and MS-MARCO. The responses vary slightly in wording but convey the same information.\",\n",
              " \"The table presents a comparison of different models (MSMARCO, Jeopardy, QGen, BART, RAG, Gold, RAG-Token, RAG-Seq) based on their performance in terms of factuality, specificity, and whether they are better, poor, or equally good. The models' performance varies, with the highest percentage for Gold in factuality (89.6%) and specificity (90.0%). The table also shows the percentage of instances where both models performed well or poorly, or where no majority was found.\",\n",
              " 'The table presents the performance metrics of different models (RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, RAG-Sequence) on various tasks (NQ, TQA, WQ, Exact Match CT, Jeopardy-QGen, MSMarco, B-1, QB-1, R-L). The metrics are presumably percentages, with higher values indicating better performance. The RAG-Sequence model shows the highest performance in most tasks, with the highest score being 90.6.',\n",
              " 'The table presents data on various tasks including Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy, Question Generation, MS-MARCO, FEVER-3-way, and FEVER-2-way. It provides specific numerical values for each task in three categories: Train, Development, and Test. The numbers range from as low as 134 to as high as 153726.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Text=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            Text.append(str(element))"
      ],
      "metadata": {
        "id": "mpbtR7DGjtQI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text elements. \\\n",
        "    Give a concise summary of the table or text that is well optimized for retrieval.text: {element} \"\"\""
      ],
      "metadata": {
        "id": "dPrD784OjxuA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(prompt_text)"
      ],
      "metadata": {
        "id": "eXNdVgH7j098"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "FTW8ZfWCj4Eb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty summaries\n",
        "\n",
        "text_summaries = []"
      ],
      "metadata": {
        "id": "Ixp_4RF-j6fN"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summaries = summarize_chain.batch(Text, {\"max_concurrency\": 5})\n"
      ],
      "metadata": {
        "id": "YS4Y-dhLj7SR"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summaries"
      ],
      "metadata": {
        "id": "JtkX0Lnwj7V9",
        "outputId": "48260c6f-531f-4626-a588-bb9f1b91cec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The text mentions two individuals, Patrick Lewis't and Ethan Perez*, but does not provide any additional information or context about them.\",\n",
              " 'The text lists the names of individuals, possibly researchers or authors, including Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, and Heinrich Küttler†.',\n",
              " 'The text lists the names of individuals: Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.',\n",
              " 'The text discusses the limitations of large pre-trained language models in accessing and manipulating knowledge, especially in knowledge-intensive tasks. It introduces Retrieval-Augmented Generation (RAG) models, which combine pre-trained parametric and non-parametric memory for language generation. These models use a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory. Two RAG formulations are compared, one that uses the same retrieved passages for the entire generated sequence, and another that uses different passages per token. The RAG models are evaluated on various NLP tasks and set new standards on three open domain QA tasks, outperforming other models. They also generate more specific, diverse, and factual language than a parametric-only seq2seq baseline.',\n",
              " 'Pre-trained neural language models can learn in-depth knowledge from data without external memory, acting as an implicit knowledge base. However, they have limitations such as difficulty in expanding or revising memory, providing insight into predictions, and may produce \"hallucinations\". Hybrid models, combining parametric memory with retrieval-based memories, can address these issues as knowledge can be directly revised, expanded, and interpreted. Models like REALM and ORQA, which combine masked language models with a differentiable retriever, have shown promising results.',\n",
              " 'The text describes an approach that combines a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator), fine-tuned end-to-end. A query x uses Maximum Inner Product Search (MIPS) to find the top-K documents zi. The final prediction y treats z as a latent variable and marginalizes over seq2seq predictions given different documents.',\n",
              " 'The text discusses the exploration of open-domain extractive question answering and the integration of hybrid parametric and non-parametric memory into sequence-to-sequence (seq2seq) models, a fundamental component of NLP.',\n",
              " 'The text discusses the development of retrieval-augmented generation (RAG) models, which combine pre-trained, parametric-memory generation models with a non-parametric memory. The parametric memory is a pre-trained seq2seq transformer, while the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. The model is trained end-to-end, with the retriever providing latent documents based on the input, and the seq2seq model generating the output based on these documents and the input. The model can be fine-tuned for any seq2seq task, with both the generator and retriever being jointly learned.',\n",
              " 'The text discusses previous work on architectures designed to enhance systems with non-parametric memory, trained specifically for certain tasks. Examples include memory networks, stack-augmented networks, and memory layers. The text contrasts this with a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge, eliminating the need for additional training.',\n",
              " 'The text discusses the benefits of combining parametric and non-parametric memory for knowledge-intensive tasks. The RAG models developed outperform other models in tasks such as open Natural Questions, WebQuestions, CuratedTrec, and TriviaQA. The models also generate more factual, specific, and diverse responses in knowledge-intensive generation tasks like MS-MARCO and Jeopardy question generation. The models also perform well in FEVER fact verification, coming close to state-of-the-art pipeline models. The non-parametric memory in the models can be updated as knowledge changes.',\n",
              " 'The text discusses RAG models that utilize an input sequence to retrieve text documents and use them as context for generating a target sequence. The model comprises two components: a retriever that returns distributions over text passages based on a query, and a generator that is parameterized.',\n",
              " 'The code for running experiments with RAG is open-sourced and part of the HuggingFace Transformers Library, available on GitHub. An interactive demo of RAG models is also accessible on the HuggingFace website.',\n",
              " 'The text discusses a function denoted by θ, which generates a current token. This process is influenced by the context of the previous i - 1 tokens (y1:i-1), the original input (x), and a retrieved passage (z).',\n",
              " 'The text discusses the training of a retriever and generator using retrieved documents as latent variables. Two models, RAG-Sequence and RAG-Token, are proposed which use these documents differently to predict target tokens. The text also introduces the components pη and pθ, and outlines the training and decoding procedure.',\n",
              " 'The RAG-Sequence model generates a complete sequence using a single retrieved document. It treats this document as a latent variable, which is marginalized to obtain the seq2seq probability through a top-K approximation. The top K documents are retrieved and the generator creates the output sequence probability for each, which are then marginalized.',\n",
              " 'The RAG-Token model allows for the selection of different latent documents for each target token, enabling the generator to use content from multiple documents when creating an answer. The top K documents are retrieved and the generator produces a distribution for the next output token for each document. This process is repeated for each subsequent output token.',\n",
              " 'RAG can be utilized for sequence classification tasks by treating the target class as a single-length target sequence, making RAG-Sequence and RAG-Token identical in this context.',\n",
              " 'The retrieval component pη(z|x) utilizes the bi-encoder architecture of Dense Passage Retrieval (DPR) as per reference [26].',\n",
              " 'The text discusses a document retrieval process using a BERTBASE document encoder to create a dense representation of a document and a query encoder for query representation. The top-k documents with the highest prior probability are calculated, which is a Maximum Inner Product Search (MIPS) problem. A pre-trained bi-encoder from DPR is used to initialize the retriever and build the document index, which was trained on TriviaQA and Natural Questions. The document index is referred to as the non-parametric memory.',\n",
              " 'The text discusses the use of BART-large, a pre-trained seq2seq transformer with 400M parameters, as a generator component in a model. The input is combined with retrieved content by concatenation. BART, pre-trained using a denoising objective and various noising functions, has achieved top results in diverse generation tasks, outperforming similar-sized T5 models. The BART generator parameters are referred to as the parametric memory.',\n",
              " 'The text discusses a method of jointly training the retriever and generator components without direct supervision on document retrieval. This process involves using a fine-tuning training corpus of input/output pairs.',\n",
              " \"The text discusses minimizing the negative marginal log-likelihood of each target using stochastic gradient descent with Adam. It mentions the costliness of updating the document encoder BERT during training, as it necessitates periodic updates to the document index, similar to REALM's pre-training. However, it asserts that this step isn't crucial for high performance. Instead, only the query encoder BERT and the BART generator are fine-tuned, keeping the document encoder and index fixed.\",\n",
              " 'The text discusses the different methods RAG-Sequence and RAG-Token use to approximate arg maxy p(y|x) during testing.',\n",
              " 'The RAG-Token model is an autoregressive seq2seq generator with a specific transition probability formula. It can be decoded using a standard beam decoder.',\n",
              " 'The RAG-Sequence uses a unique decoding procedure due to its unconventional per-token likelihood. It involves running a beam search for each document, scoring each hypothesis, and generating a set of hypotheses. The probability of each hypothesis is estimated through an additional forward pass for each document where the hypothesis does not appear in the beam. This process, known as \"Thorough Decoding\", can be time-consuming for longer output sequences. To increase efficiency, an approximation can be made to avoid additional forward passes once the candidate set has been generated, a process referred to as \"Fast Decoding\".',\n",
              " 'The text discusses an experiment with RAG for various knowledge-intensive tasks, using a single Wikipedia dump as a non-parametric knowledge source. The December 2018 dump is used, with each article divided into 100-word chunks, resulting in 21M documents. These documents are embedded using a document encoder and a MIPS index is built for fast retrieval. During training, the top k documents are retrieved for each query, with k being either 5 or 10. The value of k for testing is determined using dev data.',\n",
              " 'The text discusses open-domain question answering (QA), comparing Retrieval-Augmented Generation (RAG) to extractive QA and Closed-Book QA approaches. RAG is trained by minimizing the negative log-likelihood of answers, while extractive QA relies on non-parametric knowledge and Closed-Book QA on parametric knowledge. Four open-domain QA datasets are used: Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), and CuratedTrec (CT). The smaller CT and WQ models are initialized with the NQ RAG model. The models are evaluated using the same train/dev/test splits as previous work and Exact Match (EM) scores. TQA is also evaluated on the TQA Wiki test set.',\n",
              " \"The text discusses the use of RAG models for free-form, abstractive text generation beyond simple extractive QA. It mentions a test of RAG's natural language generation capabilities using the MSMARCO NLG task v2.1, which involves questions, retrieved passages, and annotated answers. However, only the questions and answers are used in the test.\",\n",
              " 'MSMARCO is an open-domain abstrative QA task with certain questions that require access to gold passages for accurate answers. Performance may decrease without these passages. Some questions cannot be answered using only Wikipedia, but RAG can use parametric knowledge to generate reasonable responses.',\n",
              " \"The text discusses the evaluation of RAG's generation abilities in an open-domain question generation setting. Instead of using standard open-domain QA tasks, the study proposes generating Jeopardy questions, which are more demanding. Jeopardy questions require guessing an entity based on a fact about it, making them precise and factual. The generation of these questions based on their answer entities is a challenging, knowledge-intensive task.\",\n",
              " \"The text discusses the use of SearchQA splits for training a BART model, with 100K train, 14K dev, and 27K test examples. The model's performance is evaluated using the SQuAD-tuned Q-BLEU-1 metric, which is a variant of BLEU that prioritizes matching entities. Two human evaluations are also conducted to assess the factuality and specificity of the generated content. The evaluation process involves a pairwise comparative evaluation where evaluators are shown an answer and two generated questions, and asked to choose the better one.\",\n",
              " \"FEVER is a task that involves classifying a claim based on information from Wikipedia as either supported, refuted, or unverifiable. It combines retrieval and entailment reasoning tasks, making it a suitable test for RAG models' classification abilities. FEVER class labels are mapped to single output tokens and trained with claim-class pairs, without using supervision on retrieved evidence. This approach is more applicable to real-world tasks where retrieval supervision signals may not be available. The task is explored in two variants: a standard 3-way classification (supports/refutes/not enough info) and a 2-way classification (supports/refutes), with label accuracy reported in both cases.\",\n",
              " 'Table 1 presents the superior performance of RAG in comparison to other state-of-the-art models across four open-domain QA tasks. RAG merges the benefits of \"closed-book\" and \"open-book\" approaches, achieving strong results without the need for costly, specialized pre-training. RAG\\'s retriever is initialized using DPR\\'s retriever, and it outperforms the DPR QA system, proving that neither a re-ranker nor extractive reader is necessary for top-tier performance.',\n",
              " 'The text discusses the benefits of generating answers instead of just extracting them. It highlights that documents which only hint at the answer can still contribute to generating the correct answer, a feature not possible with standard extractive methods.',\n",
              " \"The text discusses the effectiveness of RAG (Retrieval-Augmented Generation) in document marginalization. It highlights RAG's ability to generate correct answers even when the answer is not present in any retrieved document, achieving 11.8% accuracy in such cases for NQ, a feat an extractive model cannot accomplish.\",\n",
              " 'The RAG-Sequence model outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points, approaching state-of-the-art performance. Despite not having access to specific information required to generate reference answers, RAG models generate factually correct text more often and hallucinate less than BART. RAG generations also exhibit more diversity than BART generations.',\n",
              " 'RAG-Token outperforms RAG-Sequence and BART in Jeopardy question generation according to Table 2. Human evaluation results show RAG was more factual than BART in 42.7% of cases, while BART was more factual in only 7.1% of cases. Evaluators found RAG generations to be more specific. Table 3 provides examples of typical generations from each model.',\n",
              " 'The text discusses the performance of RAG-Token in generating responses for Jeopardy questions, which often contain two separate pieces of information. The model can generate responses combining content from several documents. The document posterior flattens after the first token of each book is generated, suggesting the generator can complete titles without relying on specific documents. This is supported by the BART-only baseline, which completes the generation of book titles, indicating the titles are stored in BART’s parameters. The text highlights the collaboration of parametric and non-parametric memories in guiding the generation and drawing out specific knowledge stored in the parametric memory.',\n",
              " 'Table 2 presents the performance of RAG on FEVER, achieving results within 4.3% of advanced models. These models are complex, domain-specific systems trained with intermediate retrieval supervision, a requirement not needed by RAG.',\n",
              " 'The text describes Figure 2, which illustrates the RAG-Token document posterior for each generated token for the input \"Hemingway\" in a Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".',\n",
              " \"Table 3 showcases examples from generation tasks, demonstrating that RAG models produce more specific and factually accurate responses. The table uses '?' to indicate factually incorrect responses and '*' for partially correct ones.\",\n",
              " \"The text discusses a comparison of a 2-way classification model trained by Thorne and Vlachos using RoBERTa, with the RAG model. Despite only being supplied with the claim and retrieving its own evidence, RAG's accuracy is within 2.7% of the RoBERTa model. An analysis of the documents retrieved by RAG shows a 71% overlap with gold evidence articles in the top retrieved document, and a 90% overlap in the top 10 retrieved articles.\",\n",
              " \"Section 4.3 reveals that RAG models are more factual and specific than BART for Jeopardy question generation. An investigation into generation diversity shows that RAG-Sequence's generations are more diverse than RAG-Token's, and both are significantly more diverse than BART without requiring diversity-promoting decoding.\",\n",
              " 'The text discusses the importance of the retrieval mechanism in RAG (Retrieval-Augmented Generation). It mentions an experiment where the retriever is frozen during training, and the results, as shown in Table 6, indicate that learned retrieval enhances outcomes for all tasks.',\n",
              " \"The text compares RAG's dense retriever to a BM25 retriever, replacing RAG's retriever with a fixed BM25 system. The results show that BM25 performs best for FEVER, likely due to its entity-centric claims. However, differentiable retrieval improves results on all other tasks, particularly in Open-Domain QA.\",\n",
              " 'The text discusses the advantage of non-parametric memory models like RAG over parametric-only models like T5 or BART, highlighting their ability to easily update knowledge at test time. It explains an experiment where an index was built using the DrQA Wikipedia dump from December 2016 and compared with a newer index from December 2018. The experiment involved a list of 82 world leaders who had changed.',\n",
              " 'Table 4 presents human evaluations of the Jeopardy Question Generation Task.',\n",
              " \"The text discusses the accuracy of the NQ RAG model in identifying world leaders based on the year. The model correctly identifies 70% of 2016 leaders using the 2016 index and 68% of 2018 leaders using the 2018 index. However, accuracy drops significantly when mismatched indices are used. The text suggests that updating RAG's world knowledge can be achieved by replacing its non-parametric memory.\",\n",
              " 'The text discusses the impact of retrieving more documents on the performance of models trained with either 5 or 10 latent documents. It is found that increasing the number of retrieved documents at test time improves Open-domain QA results for RAG-Sequence, with performance peaking for RAG-Token at 10 documents. However, retrieving more documents also increases Rouge-L for RAG-Token at the cost of Bleu-1, with a less significant effect on RAG-Sequence.',\n",
              " 'The text discusses the effectiveness of retrieval in enhancing performance in various Natural Language Processing (NLP) tasks such as open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. The work aims to unify these successes by demonstrating that a single retrieval-based architecture can deliver strong performance across multiple tasks.',\n",
              " 'The text discusses the evolution and success of general-purpose architectures for Natural Language Processing (NLP) tasks. It highlights the effectiveness of single, pre-trained language models in achieving strong performance on various tasks without retrieval. It mentions the success of GPT-2, BART, and T5 models, which use pre-training and bi-directional attention for improved performance. The text concludes by stating the aim to expand the range of tasks by integrating a retrieval module into pre-trained, generative language models.',\n",
              " 'The text discusses the significant work done on learning to retrieve documents in information retrieval, particularly with pre-trained, neural language models. It mentions that some work focuses on optimizing the retrieval module for specific tasks like question answering, using various methods such as search, reinforcement learning, or a latent variable approach. The text highlights that while previous successes have used different retrieval-based architectures and optimization techniques for single tasks, the authors demonstrate that a single retrieval-based architecture can be fine-tuned for strong performance across multiple tasks.',\n",
              " \"The document discusses memory-based architectures for neural networks, comparing different methods of retrieving and embedding information. The authors' approach uses raw text in the memory, making it human-readable and writable, and allowing for dynamic updates by editing the document index. This method has been used in knowledge-intensive dialog, with generators conditioned on retrieved text. The text also mentions concurrent work that learns to retrieve trained embeddings for each entity, and other work that improves dialog models' factual text generation by attending over fact embeddings.\",\n",
              " \"The text discusses Retrieve-and-Edit approaches, which involve retrieving a similar training input-output pair for a given input and editing it to provide a final output. These methods have been successful in Machine Translation and Semantic Parsing. The author's approach differs by focusing less on editing a retrieved item and more on aggregating content from multiple retrieved pieces, learning latent retrieval, and retrieving evidence documents instead of related training pairs. The text suggests that RAG techniques could be effective in these settings and represent potential future work.\",\n",
              " 'The text discusses the introduction of hybrid generation models with parametric and non-parametric memory, specifically the RAG models. These models have shown superior results in open-domain QA and are preferred over the purely parametric BART due to their factual and specific nature. The effectiveness of the learned retrieval component was validated and the ability to update the model without retraining was demonstrated. The text suggests future research into joint pre-training of the two components and explores new research directions on the interaction and effective combination of parametric and non-parametric memories in various NLP tasks.',\n",
              " 'This work provides societal benefits by being more factually accurate and controllable due to its grounding in Wikipedia. It reduces \"hallucinations\" in generations and increases interpretability. The Retriever-Augmented Generation (RAG) model can be used in various scenarios, such as answering open-domain questions on medical topics or improving job efficiency.',\n",
              " 'The text discusses potential drawbacks of using external knowledge sources like Wikipedia, which may not be entirely factual or unbiased. It also raises concerns about the use of RAG as a language model, similar to GPT-2, which could be used to generate misleading content, impersonate others, or automate spam/phishing content. The text suggests that advanced language models could automate jobs in the future. However, it also proposes the use of AI systems to combat misleading content and automated spam/phishing.',\n",
              " 'The authors express gratitude to the reviewers for their feedback, HuggingFace for assistance with open-sourcing RAG models, and Kyunghyun Cho and Sewon Min for their advice. EP acknowledges support from the NSF Graduate Research Fellowship, while PL is supported by the FAIR PhD program.',\n",
              " 'This text refers to a document or study related to Computational Linguistics, published in Florence, Italy in July 2019 by the Association for Computational Linguistics. The document can be accessed through the provided URL and is identified by the DOI: 10.18653/v1/P19-1612.',\n",
              " 'The text refers to the 2016 Approaches co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016) held in Barcelona, Spain. The proceedings of the conference are documented in volume 1773 of CEUR Workshop Proceedings and are available online.',\n",
              " \"The text discusses the testing methods for Open-domain QA, Open-MSMarco, and Jeopardy question generation. For Open-domain QA, 15 documents are retrieved for RAG-Token models and 50 for RAG-Sequence models, using Thorough Decoding. Greedy decoding is used as beam search didn't improve results. For Open-MSMarco and Jeopardy, ten documents are retrieved for both RAG-Token and RAG-Sequence, with a BART-large model trained as a baseline. A beam size of four is used, and Fast Decoding is applied for RAG-Sequence models as Thorough Decoding didn't enhance performance.\",\n",
              " 'The text describes Figure 4, which is an annotation interface used for human evaluation of factuality. Detailed instructions and examples are provided in a pop-out when \"view tool guide\" is clicked.',\n",
              " 'The text describes the user interface for human evaluation shown in Figure 4. The model corresponding to sentences A and B was randomly selected to prevent screen position bias. Annotators were encouraged to research topics online and were provided with detailed instructions and examples. Some \"gold sentences\" were included to assess annotator accuracy. Two annotators who performed poorly were excluded from the results.',\n",
              " \"The text discusses the training of RAG models and BART baselines using Fairseq, with mixed precision floating point arithmetic across 8, 32GB NVIDIA V100 GPUs. Maximum Inner Product Search with FAISS is used for fast CPU operations, requiring around 100 GB of CPU memory for all of Wikipedia. The code has been ported to HuggingFace Transformers, which is a cleaner, easier to use implementation that matches the previous version's performance. The document index is compressed using FAISS’s tools, reducing the CPU memory requirement to 36GB. Links are provided for scripts to run RAG experiments and an interactive RAG model demo.\",\n",
              " \"The text provides links to two GitHub repositories: one for PyTorch's Fairseq, a general-purpose sequence-to-sequence library, and the other for Hugging Face's Transformers, a state-of-the-art Natural Language Processing library.\",\n",
              " 'The text discusses the use of multiple answer annotations in open-domain QA for training extractive models. It mentions the use of multiple annotation examples for Natural Questions and WebQuestions in RAG, leading to increased accuracy. It also discusses the challenges in TriviaQA, where many valid answers may not be suitable for training, such as emojis or spelling variants. To address this, answer candidates that do not appear in the top 1000 documents for the query are filtered out in TriviaQA.',\n",
              " \"The text discusses the preprocessing method for CuratedTrec, which involves retrieving the top 1000 documents for each query and using the most frequently matched regex pattern as the supervision target. If no matches are found, a heuristic is used to generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace. This process is used to overcome the challenge of CuratedTrec's answers being given in regular expressions, which is considered unsuitable for answer-generation models.\",\n",
              " 'The open-domain QA community typically uses public development datasets as test datasets. The TriviaQA test dataset used is the public TriviaQA Web Development split, while some researchers use the official Wikipedia test set. The performance is found to be higher using the official Wiki test set, attributed to its questions being simpler to answer from Wikipedia. Both test sets are reported on for fair comparison.',\n",
              " 'The text discusses the process of FEVER classification, which involves regenerating a claim and classifying it using the final hidden state representation. The classification results in probabilities for three categories: \"Supported\", \"Refuted\", or \"Not Enough Info\". The text also mentions a sub-task of extracting supporting evidence from Wikipedia, which presents challenges due to the use of a different Wikipedia dump. The authors plan to address this issue in future work.',\n",
              " 'The text discusses an experiment with adding a \"Null document\" mechanism to RAG, similar to REALM, to model situations where no useful information can be retrieved. Three methods were tested: a document embedding for the null document, a static learnt bias term, and a neural network to predict the logit. However, none of these methods improved performance. In the Open MS-MARCO context, the model learns to retrieve specific documents for less beneficial questions, suggesting that null document mechanisms may not be necessary for RAG.',\n",
              " 'The text discusses RAG models which include trainable parameters for the BERT-base query and document encoder of DPR, each having 110M parameters. The document encoder is not trained by the authors. The models also contain 406M trainable parameters from BART-large, totaling 626M trainable parameters.',\n",
              " 'The best performing \"closed-book\" open-domain QA model is T5-11B with 11 billion trainable parameters. T5-large, with 770M parameters, scores 28.9 EM on Natural Questions, significantly lower than the 44.5 achieved by RAG-Sequence, suggesting that hybrid models require fewer parameters for strong performance. The non-parametric memory index, consisting of 21M 728 dimensional vectors or 15.3B values, can be stored at 8-bit floating point precision to manage memory and disk footprints.',\n",
              " 'The text discusses preliminary experiments on tasks like story generation where the retrieval component would \"collapse\" and retrieve the same documents irrespective of the input. This led to the generator learning to ignore the documents, making the RAG model perform similarly to BART. The collapse could be due to less explicit need for factual knowledge or longer target sequences. Perez et al. also found spurious retrieval results when optimizing a retrieval component for downstream tasks.',\n",
              " 'Table 7 displays the quantity of training, development, and test datapoints in each dataset.']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "gqx1Sk5Dj7bt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "    \"\"\"Getting the base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "CsR6uuEBj7g0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_summarize(img_base64, prompt):\n",
        "    \"\"\"Make image summary\"\"\"\n",
        "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
        "\n",
        "    msg = chat.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content"
      ],
      "metadata": {
        "id": "Rn4GpHeukKZX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_img_summaries(path):\n",
        "    \"\"\"\n",
        "    Generate summaries and base64 encoded strings for images\n",
        "    path: Path to list of .jpg files extracted by Unstructured\n",
        "    \"\"\"\n",
        "\n",
        "    # Store base64 encoded images\n",
        "    img_base64_list = []\n",
        "\n",
        "    # Store image summaries\n",
        "    image_summaries = []\n",
        "\n",
        "    # Prompt\n",
        "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw image. \\\n",
        "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "    # Apply to images\n",
        "    for img_file in sorted(os.listdir(path)):\n",
        "        if img_file.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(path, img_file)\n",
        "            base64_image = encode_image(img_path)\n",
        "            img_base64_list.append(base64_image)\n",
        "            image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "\n",
        "    return img_base64_list, image_summaries"
      ],
      "metadata": {
        "id": "neHcVuTBkOBL"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpath=\"/content/extracted_data\""
      ],
      "metadata": {
        "id": "qwaFM_-KkUid"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image summaries\n",
        "img_base64_list, image_summaries = generate_img_summaries(fpath)"
      ],
      "metadata": {
        "id": "8176bRF4kYEZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries"
      ],
      "metadata": {
        "id": "BP8phjjFkauT",
        "outputId": "6f405e3f-f788-4bd8-8b31-837295849aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Quiz interface asking which sentence about Hemingway is more factually true. Sentence A states \"The Sun Also Rises\" is a novel by the author of \"A Farewell to Arms.\" Sentence B claims the author of \"The Sun Also Rises\" was born in Havana, Cuba, to Spanish immigrants. Options to select include: Sentence A is more true, Sentence B is more true, Both sentences are true, Both sentences are completely untrue. Instructions on the left encourage checking the internet for accuracy.',\n",
              " 'Flowchart illustrating an end-to-end retrieval and generation system. The process includes a query encoder, non-parametric retriever, document index, and parametric generator. Examples include question answering, fact verification, and Jeopardy question generation.',\n",
              " 'Heatmap comparing text documents, with references to \"A Farewell to Arms\" and \"The Sun Also Rises\" by their titles, alongside highlighted areas representing word frequency or relevance.',\n",
              " 'Side-by-side performance graphs comparing RAG-Tok, RAG-Seq, Fixed DPR, and BM25 on NQ Exact Match, NQ Answer Recall @ K, and Bleu-1 / Rouge-L score against K Retrieved Docs.',\n",
              " 'Table displaying dataset statistics for various question answering tasks, including columns for Train, Development, and Test sets, and rows for tasks such as Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy Question Generation, MS-MARCO, FEVER-3-way, and FEVER-2-way.',\n",
              " 'Table comparing performance metrics of different models on various datasets. Models include T5-11B, T5-11B+SSM, REALM, DPR, RAG-Token, RAG-Seq, and BART. Metrics are provided for NQ, TQA, WQ, CT, Jeopardy (B-1, QB-1), MSMARCO (R-L, B-1), and FVR3 (Label Acc.). Notable performances are highlighted, with several metrics underlined or marked with asterisks.',\n",
              " 'Table summarizing the performance of different models (BART, RAG-T, RAG-S) on various tasks: MS-MARCO (defining \"middle ear\" and identifying currency needed in Scotland) and Jeopardy Question Generation (questions about Washington and \"The Divine Comedy\"). Each model\\'s generated responses are listed for comparison.',\n",
              " 'Comparison table of BART and RAG performance. The left side shows percentages for Factuality and Specificity, with RAG performing better in both categories. The right side compares performance on MSMARCO and Jeopardy QGen datasets, with RAG-Seq. achieving the highest scores after Gold standards.',\n",
              " 'Table comparing performance metrics of different RAG models (RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, RAG-Sequence) across various tasks (NQ, TQA Exact Match, WQ, CT, Jeopardy-QGen B-1/QB-1, MSMARCO R-L/B-1, FVR-3 Label Accuracy).']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "SDPR7xDukdpo"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Table=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "            Table.append(str(element))"
      ],
      "metadata": {
        "id": "6YnwlCQZkgvd"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Table"
      ],
      "metadata": {
        "id": "T_6n22dVkmGf",
        "outputId": "d7ae754e-b0d5-4e98-b100-70f279c90ab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - - /50.1 37.4 /60.5 44.7 - - Model B-1 Label Acc. Open Book REALM [20] DPR [26] 40.4 / 41.5 57.9/ - - - 40.7 46.8 41.1 50.6 SotA BART - - 15.1 19.7 49.8* 49.9* 38.2 41.6 76.8 64.0 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5',\n",
              " 'Task Input Model Generation BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. deﬁne middle ear MS- MARCO what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed in Scotland is the pound sterling. BART ?This state has the largest number of counties in the U.S. RAG-T It’s the only U.S. state named for a U.S. president RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park Jeopardy Question Gener -ation Washington BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\" The Divine Comedy',\n",
              " 'Factuality Speciﬁcity MSMARCO Jeopardy QGen BART better RAG better Both good Both poor No majority 7.1% 42.7% 11.7% 17.7% 20.8% 16.8% 37.4% 11.8% 6.9% 20.1% Gold BART RAG-Token RAG-Seq. 89.6% 70.7% 77.8% 83.5% 90.0% 32.4% 46.8% 53.8%',\n",
              " 'Model NQ TQA WQ Exact Match CT Jeopardy-QGen MSMarco B-1 B-1 QB-1 R-L RAG-Token-BM25 RAG-Sequence-BM25 29.7 31.8 41.5 44.1 32.1 36.6 33.1 33.8 17.5 11.1 22.3 19.5 55.5 56.5 48.4 46.9 75.1 91.6 RAG-Token-Frozen RAG-Sequence-Frozen 37.8 41.2 50.1 52.1 37.1 41.8 51.1 52.6 16.7 11.8 21.7 19.6 55.9 56.7 49.4 47.3 72.9 89.4 RAG-Token RAG-Sequence 43.5 44.0 54.8 55.8 46.5 44.9 51.9 53.4 17.9 15.3 22.6 21.5 56.2 57.2 49.4 47.5 74.5 90.6',\n",
              " 'Task Train Development Test Natural Questions TriviaQA WebQuestions CuratedTrec Jeopardy Question Generation MS-MARCO FEVER-3-way FEVER-2-way 79169 78786 3418 635 97392 153726 145450 96966 8758 8838 362 134 13714 12468 10000 6666 3611 11314 2033 635 26849 101093* 10000 6666']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
        "    \"\"\"\n",
        "    Create retriever that indexes summaries, but returns raw images or texts\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the storage layer\n",
        "    store = InMemoryStore()\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    # Create the multi-vector retriever\n",
        "    retriever = MultiVectorRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        docstore=store,\n",
        "        id_key=id_key,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Helper function to add documents to the vectorstore and docstore\n",
        "    def add_documents(retriever, doc_summaries, doc_contents):\n",
        "\n",
        "      doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "\n",
        "      summary_docs = [\n",
        "              Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "              for i, s in enumerate(doc_summaries)\n",
        "          ]\n",
        "      retriever.vectorstore.add_documents(summary_docs)\n",
        "      retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "      # Add texts, tables, and images\n",
        "      # Check that text_summaries is not empty before adding\n",
        "      if text_summaries:\n",
        "          add_documents(retriever, text_summaries, texts)\n",
        "      # Check that table_summaries is not empty before adding\n",
        "      if table_summaries:\n",
        "          add_documents(retriever, table_summaries, tab)\n",
        "      # Check that image_summaries is not empty before adding\n",
        "      if image_summaries:\n",
        "          add_documents(retriever, image_summaries, img)\n",
        "\n",
        "    return retriever\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"mm_rag\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever_multi_vector_img = create_multi_vector_retriever(\n",
        "    vectorstore,\n",
        "    text_summaries,\n",
        "    Text,\n",
        "    table_summaries,\n",
        "    Table,\n",
        "    image_summaries,\n",
        "    img_base64_list,\n",
        ")"
      ],
      "metadata": {
        "id": "TNglY2tVkoVk",
        "outputId": "7a97cb9e-30a1-4447-a47d-957b676e547d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector_img"
      ],
      "metadata": {
        "id": "UcA5xF6Vko1Z",
        "outputId": "1b5ba452-c5d2-47a5-b079-23f8fe7c4375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f35a5528f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f35a4862f20>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "zanQiZkQko5y"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_img_base64(img_base64):\n",
        "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
        "    # Create an HTML img tag with the base64 string as the source\n",
        "    image_html = f''\n",
        "    # Display the image by rendering the HTML\n",
        "    display(HTML(image_html))"
      ],
      "metadata": {
        "id": "UN54e1NukpA5"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_img_base64(img_base64_list[1])"
      ],
      "metadata": {
        "id": "4MQPE7LQlVS-",
        "outputId": "c9185d8f-4e9c-446a-db22-e3a33b2ea358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries[1]"
      ],
      "metadata": {
        "id": "rKMxNI1JlYRE",
        "outputId": "130c0cf8-ce78-4719-9cbb-b52e18e1f615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Flowchart illustrating an end-to-end retrieval and generation system. The process includes a query encoder, non-parametric retriever, document index, and parametric generator. Examples include question answering, fact verification, and Jeopardy question generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def looks_like_base64(sb):\n",
        "    \"\"\"Check if the string looks like base64\"\"\"\n",
        "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None"
      ],
      "metadata": {
        "id": "k_Gts_yOlb9X"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_base64_image(base64_string, size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Resize an image encoded as a Base64 string\n",
        "    \"\"\"\n",
        "    # Decode the Base64 string\n",
        "    img_data = base64.b64decode(base64_string)\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "    # Resize the image\n",
        "    resized_img = img.resize(size, Image.LANCZOS)\n",
        "\n",
        "    # Save the resized image to a bytes buffer\n",
        "    buffered = io.BytesIO()\n",
        "    resized_img.save(buffered, format=img.format)\n",
        "\n",
        "    # Encode the resized image to Base64\n",
        "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "VcsxzLY5loC6"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image_data(b64data):\n",
        "    \"\"\"\n",
        "    Check if the base64 data is an image by looking at the start of the data\n",
        "    \"\"\"\n",
        "    image_signatures = {\n",
        "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
        "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
        "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "    }\n",
        "    try:\n",
        "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
        "        for sig, format in image_signatures.items():\n",
        "            if header.startswith(sig):\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False"
      ],
      "metadata": {
        "id": "_WhxREhPlu2V"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_image_text_types(docs):\n",
        "    \"\"\"\n",
        "    Split base64-encoded images and texts\n",
        "    \"\"\"\n",
        "    b64_images = []\n",
        "    texts = []\n",
        "\n",
        "    for doc in docs:\n",
        "        # Check if the document is of type Document and extract page_content if so\n",
        "        if isinstance(doc, Document):\n",
        "            doc = doc.page_content\n",
        "        if looks_like_base64(doc) and is_image_data(doc):\n",
        "            doc = resize_base64_image(doc, size=(1300, 600))\n",
        "            b64_images.append(doc)\n",
        "        else:\n",
        "            texts.append(doc)\n",
        "\n",
        "    return {\"images\": b64_images, \"texts\": texts}"
      ],
      "metadata": {
        "id": "WQEnOf0qlgNC"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_prompt_func(data_dict):\n",
        "    \"\"\"\n",
        "    Join the context into a single string\n",
        "    \"\"\"\n",
        "    #print(data_dict)\n",
        "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "    messages = []\n",
        "\n",
        "    # Adding image(s) to the messages if present\n",
        "    if data_dict[\"context\"][\"images\"]:\n",
        "        for image in data_dict[\"context\"][\"images\"]:\n",
        "            image_message = {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "            }\n",
        "            messages.append(image_message)\n",
        "\n",
        "    # Adding the text for analysis\n",
        "    text_message = {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": (\n",
        "            \"You are a helpful assistant.\\n\"\n",
        "            \"You will be given a mixed info(s) .\\n\"\n",
        "            \"Use this information to provide relevant information to the user question. \\n\"\n",
        "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "            \"Text and / or tables:\\n\"\n",
        "            f\"{formatted_texts}\"\n",
        "        ),\n",
        "    }\n",
        "    messages.append(text_message)\n",
        "    return [HumanMessage(content=messages)]"
      ],
      "metadata": {
        "id": "Uo4ct7WLllmf"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "bubulNDzl_RU"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_rag_chain(retriever):\n",
        "    \"\"\"\n",
        "    Multi-modal RAG chain\n",
        "    \"\"\"\n",
        "\n",
        "    # Multi-modal LLM\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
        "\n",
        "\n",
        "    # RAG pipeline\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "        | RunnableLambda(img_prompt_func)\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain\n"
      ],
      "metadata": {
        "id": "sJSBgPvimCVJ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n"
      ],
      "metadata": {
        "id": "dzFPw9mqmGnM"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_multimodal_rag"
      ],
      "metadata": {
        "id": "LYCQyarVmJtf",
        "outputId": "6d6b08f2-564d-4c9e-a0b8-1db92e0bce54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  context: MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f35a5528f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f35a4862f20>)\n",
              "           | RunnableLambda(split_image_text_types),\n",
              "  question: RunnablePassthrough()\n",
              "}\n",
              "| RunnableLambda(img_prompt_func)\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f3630104dc0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f35a4863130>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1=\"Explain any images / figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
        "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\"\n",
        "\n",
        "query=\"can you explain me this Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
        "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\""
      ],
      "metadata": {
        "id": "VblLKqt0mMjQ"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RAG chain\n",
        "chain_multimodal_rag.invoke(query)"
      ],
      "metadata": {
        "id": "TR8mef8_mTIV",
        "outputId": "7a2194ac-43c7-4fed-de36-1515d3a0f164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Certainly! Let's break down the information provided and explain the performance metrics for the different datasets and tasks mentioned.\\n\\n### Left: NQ Performance as More Documents are Retrieved\\n- **NQ (Natural Questions)**: This is a dataset used for training and evaluating question-answering systems.\\n- **Performance as More Documents are Retrieved**: This likely refers to how the accuracy or effectiveness of the question-answering system changes as it retrieves more documents to find the answer. Typically, as more documents are retrieved, the system has more information to work with, which can improve its performance up to a certain point. However, retrieving too many documents might also introduce noise, which can degrade performance.\\n\\n### Center: Retrieval Recall Performance in NQ\\n- **Retrieval Recall**: This is a metric that measures the proportion of relevant documents that are successfully retrieved by the system out of all relevant documents available. \\n- **Performance in NQ**: This would indicate how well the system is able to recall relevant documents from the NQ dataset. High recall means the system is good at finding most of the relevant documents, which is crucial for ensuring that the answer to a question is among the retrieved documents.\\n\\n### Right: MS-MARCO Bleu-1 and Rouge-L as More Documents are Retrieved\\n- **MS-MARCO**: This is another dataset used for training and evaluating machine reading comprehension and question-answering systems.\\n- **Bleu-1**: This is a metric for evaluating the quality of text which measures the precision of n-grams (in this case, unigrams) between the generated text and a reference text.\\n- **Rouge-L**: This is another evaluation metric that measures the longest common subsequence between the generated text and the reference text, focusing on recall.\\n- **As More Documents are Retrieved**: Similar to the NQ performance, this refers to how the Bleu-1 and Rouge-L scores change as the system retrieves more documents. Generally, retrieving more documents can provide more context and improve these scores, but there is a balance to be struck to avoid information overload.\\n\\n### Summary\\n- **NQ Performance**: Evaluates how the system's accuracy changes with the number of documents retrieved.\\n- **Retrieval Recall in NQ**: Measures the system's ability to find relevant documents in the NQ dataset.\\n- **MS-MARCO Bleu-1 and Rouge-L**: Assesses the quality of the generated answers based on precision and recall metrics as more documents are retrieved.\\n\\nIn essence, these metrics and evaluations help in understanding how well a question-answering system performs in terms of retrieving relevant documents and generating accurate answers as the number of documents it considers increases.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wAwsJdFXmVd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}